# Use mpirun to run 16 processes (2 nodes x 8 GPUs each) across 2 worker pods, each with 8 GPUs.

# The launcher pod uses SSH to reach each worker pod.

# A MPI Job contain: 1 launcher pod and N worker pods.

# ----- SSH Key Exchange (Operator's Role) ----- 
# When the MPIJob is created, the Operator automatically generates an SSH Key Pair (public and private keys) for the job.
# The private key is injected into the Launcher Pod's filesystem (typically at /root/.ssh/id_rsa).
# The public key is injected into all Worker Pods' filesystems and appended to their authorized keys file (typically at /root/.ssh/authorized_keys).

# ----- Network Communication and Discovery ----- 

# Kubernetes ensures the Launcher can find and connect to the Workers via a Service object created by the MPI Operator:
# The MPI Operator creates a Kubernetes Headless Service that targets all Worker Pods. This service exposes the network addresses of the Workers.
# The hostname for each Worker Pod is injected into the Launcher Pod's DNS resolution system. The Worker Pods are typically named in the format: $(MPI_JOB_NAME)-worker-$(INDEX).
# The Launcher uses these hostnames to resolve the IP addresses of the Workers.


apiVersion: kubeflow.org/v2beta1
kind: MPIJob # Custom Resource Definition for Kubeflow MPI Operator, used to run MPI jobs across multiple nodes
metadata:
  name: mpi-multus-gfx942 # Name of the MPI job
spec:
  slotsPerWorker: 8 # 8 slots (MPI processes) / worker pod, one MPI process per GPU
  runPolicy:
    cleanPodPolicy: Running # Keep the pods around (not deleted) after job completion 
  mpiReplicaSpecs:

    # Launcher pod specification - the single entry point for the MPI job
    Launcher:
      replicas: 1 # Single launcher pod to initiate the MPI job
      template:
        spec:
          containers:
          - name: mpi-launcher
            image: docker.io/richardxgf/amd:rccl-test-325 # Replace with own image created from Dockerfile
            command: # run the mpirun command with appropriate arguments
            - mpirun
            - --allow-run-as-root
            - -np
            - "16"                     # 16 total processes (2 nodes x 8 GPUs each)
            - -bind-to
            - none
            - -map-by
            - slot
            - -x
            - NCCL_SOCKET_IFNAME=eth0  # Use eth0 for communication
            - -x
            - NCCL_CROSS_NIC=0         # Disable cross NIC communication
            - -x
            - NCCL_PXN_DISABLE=0
            - -x
            - NCCL_NET_DISABLE_INTRA=1 # Force inter-node traffic to use the dedicated fabric instead of the standard Ethernet.
            - -x
            - NCCL_IB_GID_INDEX=1
            - -x
            - NCCL_IB_TC=104
            - -x
            - PATH
            - -mca
            - plm_rsh_args
            - "-p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"  # SSH options for MPI
            - -mca
            - btl
            - self,tcp
            - -mca
            - pml
            - ^ucx
            - /workspace/rccl-tests/build/all_reduce_perf
            - -b
            - "8"
            - -e
            - 16G
            - -f
            - "2"
            - -g
            - "1"
            - -c
            - "1"
            - -n
            - "100"




    # Worker pod specification
    Worker:
      replicas: 2 # 2 worker pods 
      template:
        metadata:
          annotations:
            k8s.v1.cni.cncf.io/networks: >- # Attach multiple RoCE network interfaces to each worker using Multus CNI
              roce-net-fabric0@fabric0,
              roce-net-fabric1@fabric1,
              roce-net-fabric2@fabric2,
              roce-net-fabric3@fabric3,
              roce-net-fabric4@fabric4,
              roce-net-fabric5@fabric5,
              roce-net-fabric6@fabric6,
              roce-net-fabric7@fabric7
        spec:
          nodeSelector:
            doks.digitalocean.com/gpu-model: mi325x # Schedule pods on nodes with mi325x GPUs
          tolerations:
          - key: "amd.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
          imagePullSecrets:
            - name: do-registry

          initContainers:     # Generates SSH host keys inside the pod before MPI starts.
          - name: setup-ssh   # Run as an init container to set up SSH keys and configuration  
            image: docker.io/richardxgf/amd:rccl-test-325 # Replace with own image created from Dockerfile
            command:   # generate SSH host keys and configuration and store them in a shared volume
            - /bin/bash
            - -c
            - |
              set -ex
              
              echo "Setting up SSH configuration..."
              mkdir -p /ssh-setup/sshd
              
              echo "Generating SSH host keys..."
              ssh-keygen -t rsa -f /ssh-setup/sshd/ssh_host_rsa_key -N '' 2>/dev/null
              ssh-keygen -t ecdsa -f /ssh-setup/sshd/ssh_host_ecdsa_key -N '' 2>/dev/null
              ssh-keygen -t ed25519 -f /ssh-setup/sshd/ssh_host_ed25519_key -N '' 2>/dev/null
              
              cat > /ssh-setup/sshd/sshd_config << 'SSHD_EOF'
              Port 2222
              HostKey /etc/ssh-runtime/ssh_host_rsa_key
              HostKey /etc/ssh-runtime/ssh_host_ecdsa_key
              HostKey /etc/ssh-runtime/ssh_host_ed25519_key
              PermitRootLogin yes
              PubkeyAuthentication yes
              AuthorizedKeysFile /root/.ssh/authorized_keys /root/.ssh/id_rsa.pub
              PasswordAuthentication no
              ChallengeResponseAuthentication no
              UsePAM no
              PrintMotd no
              PidFile /var/run/sshd.pid
              StrictModes no
              SSHD_EOF
              
              echo "SSH setup complete"
              echo "Files in /ssh-setup/sshd:"
              ls -la /ssh-setup/sshd/
            volumeMounts:
            - name: ssh-setup
              mountPath: /ssh-setup

          containers:
          - name: mpi-worker # The main worker container that runs the MPI processes
            image: docker.io/richardxgf/amd:rccl-test-325 # Replace with own image created from Dockerfile
            command:  # Run the SSH daemon in the foreground to allow SSH access
            - /bin/bash
            - -c
            - |
              set -ex
              
              echo "Starting worker container..."
              mkdir -p /var/run/sshd /etc/ssh-runtime
              
              echo "Copying SSH host keys and config from init container..."
              cp /ssh-setup/sshd/* /etc/ssh-runtime/
              
              echo "SSH runtime files:"
              ls -la /etc/ssh-runtime/
              
              echo "Verifying operator-provided SSH keys at /root/.ssh..."
              ls -la /root/.ssh/ || echo "Warning: /root/.ssh not found"
              
              echo "Starting sshd in foreground..."
              exec /usr/sbin/sshd -D -e -f /etc/ssh-runtime/sshd_config
            securityContext:
              privileged: true
              capabilities:
                add:
                - IPC_LOCK
            volumeMounts:
            - name: ssh-setup
              mountPath: /ssh-setup 
            - name: dshm
              mountPath: /dev/shm
            resources:
              limits:
                amd.com/gpu: 8
                rdma/fabric0: 1
                rdma/fabric1: 1
                rdma/fabric2: 1
                rdma/fabric3: 1
                rdma/fabric4: 1
                rdma/fabric5: 1
                rdma/fabric6: 1
                rdma/fabric7: 1
              requests:
                amd.com/gpu: 8
                rdma/fabric0: 1
                rdma/fabric1: 1
                rdma/fabric2: 1
                rdma/fabric3: 1
                rdma/fabric4: 1
                rdma/fabric5: 1
                rdma/fabric6: 1
                rdma/fabric7: 1
            readinessProbe:
              tcpSocket:
                port: 2222
              initialDelaySeconds: 5
              periodSeconds: 3
              timeoutSeconds: 2
            livenessProbe:
              tcpSocket:
                port: 2222
              initialDelaySeconds: 15
              periodSeconds: 10
              timeoutSeconds: 2
          volumes:             # for both init and main containers
          - name: ssh-setup
            emptyDir: {}       # The type of volume - temporary storage, lasting as long as the pod is running
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: 16Gi
